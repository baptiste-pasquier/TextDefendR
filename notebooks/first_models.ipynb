{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from nlp_adversarial_attacks.reactdetect.utils.magic_vars import PRIMARY_KEY_FIELDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_pickle = \"../data_tcab/whole_feature_dataset_with_canine.pickle\"\n",
    "objective = \"binary\"\n",
    "model_type = \"xgboost\"\n",
    "feature_set = \"tlc\"\n",
    "scaler = \"StandardScaler\"\n",
    "pca = False\n",
    "n_trials = 10\n",
    "n_jobs = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df = pd.read_pickle(path_to_pickle)\n",
    "df.dropna(inplace=True)\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "# Separate the different features\n",
    "tp_features = df.columns[df.columns.str.startswith(\"tp_\")].tolist()\n",
    "tp_bert_features = df.columns[df.columns.str.startswith(\"tp_bert_\")].tolist()\n",
    "lm_features = df.columns[df.columns.str.startswith(\"lm_\")].tolist()\n",
    "tm_features = df.columns[df.columns.str.startswith(\"tm_\")].tolist()\n",
    "canine_features = df.columns[df.columns.str.startswith(\"canine_tp_bert_\")].tolist()\n",
    "feature_sets = {\n",
    "    \"bert\": tp_bert_features,\n",
    "    \"t\": tp_features,\n",
    "    \"tl\": tp_features + lm_features,\n",
    "    \"tlc\": tp_features + lm_features + tm_features,\n",
    "    \"canine\": canine_features,\n",
    "    \"tlc_canine\": tp_features + lm_features + tm_features + canine_features,\n",
    "}\n",
    "features = feature_sets[feature_set]\n",
    "\n",
    "# Split the data into X and y\n",
    "var_df = df.loc[:, features]\n",
    "id_vars = [\"unique_id\"] + PRIMARY_KEY_FIELDS\n",
    "index_df = df.loc[:, id_vars]\n",
    "index_df[\"label\"] = np.where(index_df[\"attack_name\"] == \"clean\", \"clean\", \"attack\")\n",
    "del df\n",
    "\n",
    "# Split train and test sets\n",
    "label_var = \"label\" if objective == \"binary\" else \"attack_name\"\n",
    "train_idx, test_idx = train_test_split(\n",
    "    var_df.index,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    ")\n",
    "X_train, X_test = var_df.loc[train_idx], var_df.loc[test_idx]\n",
    "y_train, y_test = index_df.loc[train_idx, label_var], index_df.loc[test_idx, label_var]\n",
    "del var_df\n",
    "\n",
    "# Encode the labels\n",
    "le = LabelEncoder().fit(y_train)\n",
    "y_train_enc, y_test_enc = le.transform(y_train), le.transform(y_test)\n",
    "int_to_label = {i: label for i, label in enumerate(le.classes_)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the objective function\n",
    "def objective_function(trial):\n",
    "    if model_type == \"lr\":\n",
    "        lr_c = trial.suggest_float(\"lr_c\", 1e-5, 1e5, log=True)\n",
    "        classifier_obj = sklearn.linear_model.LogisticRegression(C=lr_c, max_iter=100)\n",
    "    elif model_type == \"xgboost\":\n",
    "        xgb_max_depth = trial.suggest_int(\"xgb_max_depth\", 2, 16, log=True)\n",
    "        xgb_n_estimators = trial.suggest_int(\"xgb_n_estimators\", 10, 100, log=True)\n",
    "        classifier_obj = XGBClassifier(\n",
    "            max_depth=xgb_max_depth, n_estimators=xgb_n_estimators\n",
    "        )\n",
    "\n",
    "    # Preprocessing steps (scaler and PCA)\n",
    "    print(\"-- Building pipeline\")\n",
    "    steps = []\n",
    "    if scaler == \"None\":\n",
    "        steps.append((\"scaler\", \"passthrough\"))\n",
    "    elif scaler == \"StandardScaler\":\n",
    "        steps.append((\"scaler\", StandardScaler()))\n",
    "    elif scaler == \"MinMaxScaler\":\n",
    "        steps.append((\"scaler\", MinMaxScaler()))\n",
    "\n",
    "    if pca:\n",
    "        pca_n_components = trial.suggest_int(\"pca_n_components\", 25, 250, log=True)\n",
    "        steps.append((\"pca\", PCA(n_components=pca_n_components)))\n",
    "\n",
    "    steps.append((\"classifier\", classifier_obj))\n",
    "    pipe = Pipeline(steps)\n",
    "    print(pipe)\n",
    "\n",
    "    print(\"-- Training\")\n",
    "    pipe.fit(X_train, y_train_enc)\n",
    "\n",
    "    print(\"-- Evaluating\")\n",
    "    y_pred = pipe.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test_enc, y_pred)\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-15 20:09:55,359]\u001b[0m A new study created in RDB with name: binary_xgboost_tlc_standardscaler_nopca\u001b[0m\n",
      "/home/gwatk/.cache/pypoetry/virtualenvs/nlp-adversarial-attacks-kOPV8VHT-py3.10/lib/python3.10/site-packages/optuna/progress_bar.py:56: ExperimentalWarning: Progress bar is experimental (supported from v1.2.0). The interface can change in the future.\n",
      "  self._init_valid()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdec3cb8d5634fbeac44917bc3f11cc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Building pipeline\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('classifier',\n",
      "                 XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "                               colsample_bylevel=None, colsample_bynode=None,\n",
      "                               colsample_bytree=None,\n",
      "                               early_stopping_rounds=None,\n",
      "                               enable_categorical=False, eval_metric=None,\n",
      "                               feature_types=None, gamma=None, gpu_id=None,\n",
      "                               grow_policy=None, importance_type=None,\n",
      "                               interaction_constraints=None, learning_rate=None,\n",
      "                               max_bin=None, max_cat_threshold=None,\n",
      "                               max_cat_to_onehot=None, max_delta_step=None,\n",
      "                               max_depth=8, max_leaves=None,\n",
      "                               min_child_weight=None, missing=nan,\n",
      "                               monotone_constraints=None, n_estimators=17,\n",
      "                               n_jobs=None, num_parallel_tree=None,\n",
      "                               predictor=None, random_state=None, ...))])\n",
      "-- Training\n",
      "-- Building pipeline\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('classifier',\n",
      "                 XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "                               colsample_bylevel=None, colsample_bynode=None,\n",
      "                               colsample_bytree=None,\n",
      "                               early_stopping_rounds=None,\n",
      "                               enable_categorical=False, eval_metric=None,\n",
      "                               feature_types=None, gamma=None, gpu_id=None,\n",
      "                               grow_policy=None, importance_type=None,\n",
      "                               interaction_constraints=None, learning_rate=None,\n",
      "                               max_bin=None, max_cat_threshold=None,\n",
      "                               max_cat_to_onehot=None, max_delta_step=None,\n",
      "                               max_depth=4, max_leaves=None,\n",
      "                               min_child_weight=None, missing=nan,\n",
      "                               monotone_constraints=None, n_estimators=12,\n",
      "                               n_jobs=None, num_parallel_tree=None,\n",
      "                               predictor=None, random_state=None, ...))])\n",
      "-- Training\n",
      "-- Building pipeline\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('classifier',\n",
      "                 XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "                               colsample_bylevel=None, colsample_bynode=None,\n",
      "                               colsample_bytree=None,\n",
      "                               early_stopping_rounds=None,\n",
      "                               enable_categorical=False, eval_metric=None,\n",
      "                               feature_types=None, gamma=None, gpu_id=None,\n",
      "                               grow_policy=None, importance_type=None,\n",
      "                               interaction_constraints=None, learning_rate=None,\n",
      "                               max_bin=None, max_cat_threshold=None,\n",
      "                               max_cat_to_onehot=None, max_delta_step=None,\n",
      "                               max_depth=12, max_leaves=None,\n",
      "                               min_child_weight=None, missing=nan,\n",
      "                               monotone_constraints=None, n_estimators=85,\n",
      "                               n_jobs=None, num_parallel_tree=None,\n",
      "                               predictor=None, random_state=None, ...))])\n",
      "-- Training\n",
      "-- Building pipeline\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('classifier',\n",
      "                 XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "                               colsample_bylevel=None, colsample_bynode=None,\n",
      "                               colsample_bytree=None,\n",
      "                               early_stopping_rounds=None,\n",
      "                               enable_categorical=False, eval_metric=None,\n",
      "                               feature_types=None, gamma=None, gpu_id=None,\n",
      "                               grow_policy=None, importance_type=None,\n",
      "                               interaction_constraints=None, learning_rate=None,\n",
      "                               max_bin=None, max_cat_threshold=None,\n",
      "                               max_cat_to_onehot=None, max_delta_step=None,\n",
      "                               max_depth=2, max_leaves=None,\n",
      "                               min_child_weight=None, missing=nan,\n",
      "                               monotone_constraints=None, n_estimators=11,\n",
      "                               n_jobs=None, num_parallel_tree=None,\n",
      "                               predictor=None, random_state=None, ...))])\n",
      "-- Training\n",
      "-- Building pipeline\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('classifier',\n",
      "                 XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "                               colsample_bylevel=None, colsample_bynode=None,\n",
      "                               colsample_bytree=None,\n",
      "                               early_stopping_rounds=None,\n",
      "                               enable_categorical=False, eval_metric=None,\n",
      "                               feature_types=None, gamma=None, gpu_id=None,\n",
      "                               grow_policy=None, importance_type=None,\n",
      "                               interaction_constraints=None, learning_rate=None,\n",
      "                               max_bin=None, max_cat_threshold=None,\n",
      "                               max_cat_to_onehot=None, max_delta_step=None,\n",
      "                               max_depth=5, max_leaves=None,\n",
      "                               min_child_weight=None, missing=nan,\n",
      "                               monotone_constraints=None, n_estimators=11,\n",
      "                               n_jobs=None, num_parallel_tree=None,\n",
      "                               predictor=None, random_state=None, ...))])\n",
      "-- Training\n",
      "-- Building pipeline\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('classifier',\n",
      "                 XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "                               colsample_bylevel=None, colsample_bynode=None,\n",
      "                               colsample_bytree=None,\n",
      "                               early_stopping_rounds=None,\n",
      "                               enable_categorical=False, eval_metric=None,\n",
      "                               feature_types=None, gamma=None, gpu_id=None,\n",
      "                               grow_policy=None, importance_type=None,\n",
      "                               interaction_constraints=None, learning_rate=None,\n",
      "                               max_bin=None, max_cat_threshold=None,\n",
      "                               max_cat_to_onehot=None, max_delta_step=None,\n",
      "                               max_depth=2, max_leaves=None,\n",
      "                               min_child_weight=None, missing=nan,\n",
      "                               monotone_constraints=None, n_estimators=56,\n",
      "                               n_jobs=None, num_parallel_tree=None,\n",
      "                               predictor=None, random_state=None, ...))])\n",
      "-- Training\n",
      "-- Building pipeline\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('classifier',\n",
      "                 XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "                               colsample_bylevel=None, colsample_bynode=None,\n",
      "                               colsample_bytree=None,\n",
      "                               early_stopping_rounds=None,\n",
      "                               enable_categorical=False, eval_metric=None,\n",
      "                               feature_types=None, gamma=None, gpu_id=None,\n",
      "                               grow_policy=None, importance_type=None,\n",
      "                               interaction_constraints=None, learning_rate=None,\n",
      "                               max_bin=None, max_cat_threshold=None,\n",
      "                               max_cat_to_onehot=None, max_delta_step=None,\n",
      "                               max_depth=2, max_leaves=None,\n",
      "                               min_child_weight=None, missing=nan,\n",
      "                               monotone_constraints=None, n_estimators=12,\n",
      "                               n_jobs=None, num_parallel_tree=None,\n",
      "                               predictor=None, random_state=None, ...))])\n",
      "-- Training\n",
      "-- Building pipeline\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('classifier',\n",
      "                 XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "                               colsample_bylevel=None, colsample_bynode=None,\n",
      "                               colsample_bytree=None,\n",
      "                               early_stopping_rounds=None,\n",
      "                               enable_categorical=False, eval_metric=None,\n",
      "                               feature_types=None, gamma=None, gpu_id=None,\n",
      "                               grow_policy=None, importance_type=None,\n",
      "                               interaction_constraints=None, learning_rate=None,\n",
      "                               max_bin=None, max_cat_threshold=None,\n",
      "                               max_cat_to_onehot=None, max_delta_step=None,\n",
      "                               max_depth=2, max_leaves=None,\n",
      "                               min_child_weight=None, missing=nan,\n",
      "                               monotone_constraints=None, n_estimators=13,\n",
      "                               n_jobs=None, num_parallel_tree=None,\n",
      "                               predictor=None, random_state=None, ...))])\n",
      "-- Training\n",
      "-- Building pipeline\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('classifier',\n",
      "                 XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "                               colsample_bylevel=None, colsample_bynode=None,\n",
      "                               colsample_bytree=None,\n",
      "                               early_stopping_rounds=None,\n",
      "                               enable_categorical=False, eval_metric=None,\n",
      "                               feature_types=None, gamma=None, gpu_id=None,\n",
      "                               grow_policy=None, importance_type=None,\n",
      "                               interaction_constraints=None, learning_rate=None,\n",
      "                               max_bin=None, max_cat_threshold=None,\n",
      "                               max_cat_to_onehot=None, max_delta_step=None,\n",
      "                               max_depth=5, max_leaves=None,\n",
      "                               min_child_weight=None, missing=nan,\n",
      "                               monotone_constraints=None, n_estimators=24,\n",
      "                               n_jobs=None, num_parallel_tree=None,\n",
      "                               predictor=None, random_state=None, ...))])\n",
      "-- Training\n",
      "-- Building pipeline\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('classifier',\n",
      "                 XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "                               colsample_bylevel=None, colsample_bynode=None,\n",
      "                               colsample_bytree=None,\n",
      "                               early_stopping_rounds=None,\n",
      "                               enable_categorical=False, eval_metric=None,\n",
      "                               feature_types=None, gamma=None, gpu_id=None,\n",
      "                               grow_policy=None, importance_type=None,\n",
      "                               interaction_constraints=None, learning_rate=None,\n",
      "                               max_bin=None, max_cat_threshold=None,\n",
      "                               max_cat_to_onehot=None, max_delta_step=None,\n",
      "                               max_depth=2, max_leaves=None,\n",
      "                               min_child_weight=None, missing=nan,\n",
      "                               monotone_constraints=None, n_estimators=29,\n",
      "                               n_jobs=None, num_parallel_tree=None,\n",
      "                               predictor=None, random_state=None, ...))])\n",
      "-- Training\n"
     ]
    }
   ],
   "source": [
    "study_name = f\"{objective}_{model_type}_{feature_set}_{scaler.lower()}_{'pca' if pca else 'nopca'}\"\n",
    "storage_name = f\"sqlite:///{study_name}.db\"\n",
    "study = optuna.create_study(\n",
    "    study_name=study_name,\n",
    "    storage=storage_name,\n",
    "    load_if_exists=True,\n",
    "    direction=\"maximize\",\n",
    ")\n",
    "# study = optuna.create_study(study_name=study_name, direction=\"maximize\")\n",
    "study.optimize(\n",
    "    objective_function, n_trials=n_trials, show_progress_bar=True, n_jobs=n_jobs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"../data_tcab/whole_feature_dataset.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_features = df.columns[df.columns.str.startswith(\"tp_\")].tolist()\n",
    "tp_bert_features = df.columns[df.columns.str.startswith(\"tp_bert_\")].tolist()\n",
    "lm_features = df.columns[df.columns.str.startswith(\"lm_\")].tolist()\n",
    "tm_features = df.columns[df.columns.str.startswith(\"tm_\")].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_vars = [\n",
    "    \"unique_id\",\n",
    "    \"attack_name\",\n",
    "    \"attack_toolchain\",\n",
    "    \"attack_id\",\n",
    "    \"scenario\",\n",
    "    \"target_model\",\n",
    "    \"target_model_dataset\",\n",
    "    \"attack_id_bis\",\n",
    "]\n",
    "index_df = df.loc[:, id_vars]\n",
    "index_df[\"label\"] = np.where(index_df[\"attack_name\"] == \"clean\", 0, 1)\n",
    "var_df = df.drop(id_vars, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx, test_idx, y_train, y_test = train_test_split(\n",
    "    var_df.index,\n",
    "    index_df[\"label\"],\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=index_df[\"label\"],\n",
    ")\n",
    "X_train, X_test = var_df.loc[train_idx], var_df.loc[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = make_pipeline(\n",
    "    StandardScaler(), PCA(100), LogisticRegression(max_iter=1000, random_state=42)\n",
    ")\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "y_pred = pipe.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define an objective function to be maximized.\n",
    "def objective(trial):\n",
    "    feature_subset = trial.suggest_categorical(\n",
    "        \"feature_subset\",\n",
    "        [\"tp_features\", \"tp_bert_features\", \"lm_features\", \"tm_features\"],\n",
    "    )\n",
    "    feature_dict = {\n",
    "        \"tp_bert_features\": tp_bert_features,\n",
    "        \"tp_features\": tp_features,\n",
    "        \"lm_features\": tp_features + lm_features,\n",
    "        \"tm_features\": tp_features + lm_features + tm_features,\n",
    "    }\n",
    "    feature_selector = ColumnTransformer(\n",
    "        transformers=[(\"selector\", \"passthrough\", feature_dict[feature_subset])],\n",
    "        remainder=\"drop\",\n",
    "    )\n",
    "\n",
    "    classifier_name = trial.suggest_categorical(\n",
    "        \"classifier\", [\"LogisticRegression\", \"XGBoost\"]\n",
    "    )\n",
    "    if classifier_name == \"LogisticRegression\":\n",
    "        lr_c = trial.suggest_float(\"lr_c\", 1e-10, 1e10, log=True)\n",
    "        lr_penalty = trial.suggest_categorical(\"lr_penalty\", [None, \"l1\", \"l2\"])\n",
    "        classifier_obj = sklearn.linear_model.LogisticRegression(\n",
    "            C=lr_c, penalty=lr_penalty, solver=\"saga\", max_iter=1000\n",
    "        )\n",
    "    elif classifier_name == \"XGBoost\":\n",
    "        xgb_max_depth = trial.suggest_int(\"xgb_max_depth\", 2, 32, log=True)\n",
    "        xgb_n_estimators = trial.suggest_int(\"xgb_n_estimators\", 10, 500, log=True)\n",
    "        classifier_obj = XGBClassifier(\n",
    "            max_depth=xgb_max_depth, n_estimators=xgb_n_estimators\n",
    "        )\n",
    "\n",
    "    scaler_name = trial.suggest_categorical(\n",
    "        \"scaler\", [\"None\", \"StandardScaler\", \"StandardScaler+PCA\"]\n",
    "    )\n",
    "    if scaler_name == \"None\":\n",
    "        preprocessor = Pipeline([(\"preprocessor\", \"passthrough\")])\n",
    "    elif scaler_name == \"StandardScaler\":\n",
    "        preprocessor = Pipeline([(\"scaler\", StandardScaler())])\n",
    "    elif scaler_name == \"StandardScaler+PCA\":\n",
    "        pca_n_components = trial.suggest_int(\"pca_n_components\", 25, 250, log=True)\n",
    "        preprocessor = Pipeline(\n",
    "            [(\"scaler\", StandardScaler()), (\"pca\", PCA(n_components=pca_n_components))]\n",
    "        )\n",
    "\n",
    "    pipe = Pipeline(\n",
    "        [\n",
    "            (\"selector\", feature_selector),\n",
    "            (\"preprocessor\", preprocessor),\n",
    "            (\"classifier\", classifier_obj),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    pipe.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = pipe.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Create a study object and optimize the objective function.\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=100, show_progress_bar=True, n_jobs=-1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-adversarial-attacks-qj5liwpn-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
