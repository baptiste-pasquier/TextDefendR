{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate with Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 3080\n",
      "Free memory : 8.9 / 10.0 GB\n",
      "\n",
      "Using TF32\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from optimum.bettertransformer import BetterTransformer\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from transformers.utils import is_torch_tf32_available\n",
    "\n",
    "assert torch.cuda.is_available()\n",
    "\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print(\n",
    "    f\"Free memory : {round(torch.cuda.mem_get_info()[0] / 1024 ** 3,1)} / {round(torch.cuda.mem_get_info()[1] / 1024 ** 3,1)} GB\"\n",
    ")\n",
    "\n",
    "if is_torch_tf32_available():\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    print(\"\\nUsing TF32\")\n",
    "else:\n",
    "    print(\"\\nTF32 not available\")\n",
    "\n",
    "t = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "dataset_path = \"allocine\"\n",
    "input_column = \"review\"\n",
    "label_column = \"label\"\n",
    "train_split = \"train\"\n",
    "eval_split = \"validation\"\n",
    "test_split = \"test\"\n",
    "\n",
    "# Model\n",
    "model_checkpoint = \"baptiste-pasquier/distilcamembert-allocine\"\n",
    "\n",
    "# Training\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\".\",\n",
    "    per_device_eval_batch_size=16,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "# Metrics\n",
    "metrics = [\"accuracy\", \"f1\", \"precision\", \"recall\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset allocine (C:\\Users\\Baptiste\\.cache\\huggingface\\datasets\\allocine\\allocine\\1.0.0\\ea86b1dc05eae3a45a07b6281f2d4033b5fe7927b1008d06aa457ca1eae660d0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be2174ab874e4d4da95773f340eac1bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(dataset_path)\n",
    "splits = dataset[train_split].info.splits.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\Baptiste\\.cache\\huggingface\\datasets\\allocine\\allocine\\1.0.0\\ea86b1dc05eae3a45a07b6281f2d4033b5fe7927b1008d06aa457ca1eae660d0\\cache-2b4071c7069eb28b.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Baptiste\\.cache\\huggingface\\datasets\\allocine\\allocine\\1.0.0\\ea86b1dc05eae3a45a07b6281f2d4033b5fe7927b1008d06aa457ca1eae660d0\\cache-508123078501f427.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Baptiste\\.cache\\huggingface\\datasets\\allocine\\allocine\\1.0.0\\ea86b1dc05eae3a45a07b6281f2d4033b5fe7927b1008d06aa457ca1eae660d0\\cache-04b447737d030e4f.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a936c4caa828448f8b8bc69cd13e5817",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/160 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\Baptiste\\.cache\\huggingface\\datasets\\allocine\\allocine\\1.0.0\\ea86b1dc05eae3a45a07b6281f2d4033b5fe7927b1008d06aa457ca1eae660d0\\cache-4de7aa5c909b35c0.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Baptiste\\.cache\\huggingface\\datasets\\allocine\\allocine\\1.0.0\\ea86b1dc05eae3a45a07b6281f2d4033b5fe7927b1008d06aa457ca1eae660d0\\cache-5b3760e821f50a0b.arrow\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[input_column], truncation=True)\n",
    "\n",
    "\n",
    "encoded_dataset = dataset.map(tokenize_function, batched=True)\n",
    "encoded_dataset = dataset.map(\n",
    "    tokenize_function, batched=True, remove_columns=[input_column]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)\n",
    "model = BetterTransformer.transform(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_metrics = evaluate.combine(metrics)\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return clf_metrics.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 160000\n",
      "  Batch size = 16\n",
      "You're using a CamembertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "c:\\Users\\Baptiste\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\nlp-adversarial-attacks-ghBt6cj_-py3.10\\lib\\site-packages\\optimum\\bettertransformer\\models\\encoder_models.py:207: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. (Triggered internally at ..\\aten\\src\\ATen\\NestedTensorImpl.cpp:177.)\n",
      "  hidden_states = torch._nested_tensor_from_mask(hidden_states, ~attention_mask)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "055b17e33f2446339e6a5be6be35ebb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 20000\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af7b36dd91f2460dad9ae783ebdfd578",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 20000\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92422342c1524402970c18c457be4655",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = []\n",
    "for split in splits:\n",
    "    eval_metrics = trainer.evaluate(encoded_dataset[split])\n",
    "    results.append(eval_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_accuracy</th>\n",
       "      <th>eval_f1</th>\n",
       "      <th>eval_precision</th>\n",
       "      <th>eval_recall</th>\n",
       "      <th>eval_runtime</th>\n",
       "      <th>eval_samples_per_second</th>\n",
       "      <th>eval_steps_per_second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train</th>\n",
       "      <td>0.022919</td>\n",
       "      <td>0.993712</td>\n",
       "      <td>0.993779</td>\n",
       "      <td>0.990460</td>\n",
       "      <td>0.997121</td>\n",
       "      <td>207.3856</td>\n",
       "      <td>771.510</td>\n",
       "      <td>48.219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>validation</th>\n",
       "      <td>0.102344</td>\n",
       "      <td>0.970700</td>\n",
       "      <td>0.970311</td>\n",
       "      <td>0.963186</td>\n",
       "      <td>0.977542</td>\n",
       "      <td>25.4372</td>\n",
       "      <td>786.250</td>\n",
       "      <td>49.141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <td>0.106034</td>\n",
       "      <td>0.970800</td>\n",
       "      <td>0.969738</td>\n",
       "      <td>0.964043</td>\n",
       "      <td>0.975500</td>\n",
       "      <td>26.2675</td>\n",
       "      <td>761.396</td>\n",
       "      <td>47.587</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            eval_loss  eval_accuracy   eval_f1  eval_precision  eval_recall  \\\n",
       "train        0.022919       0.993712  0.993779        0.990460     0.997121   \n",
       "validation   0.102344       0.970700  0.970311        0.963186     0.977542   \n",
       "test         0.106034       0.970800  0.969738        0.964043     0.975500   \n",
       "\n",
       "            eval_runtime  eval_samples_per_second  eval_steps_per_second  \n",
       "train           207.3856                  771.510                 48.219  \n",
       "validation       25.4372                  786.250                 49.141  \n",
       "test             26.2675                  761.396                 47.587  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(results, index=splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 00:04:36\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total time: {time.strftime('%H:%M:%S', time.gmtime(time.time()-t))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
